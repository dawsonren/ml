# February 22nd, 2025

Read through [this link](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53) to understand the mel-spectrogram.
Learned:
1. A signal captures the amplitude over time, it comes out of a microphone. Intuitively, it captures the position of a diaphragm at a bunch of sample points. The sample rate is usually 44.1 kHz, so 44,100 samples per second.
2. A spectrum is a transform of the signal that represents the individual average amplitudes of each frequency captured over the time of the signal. This is done with a fourier transform.
3. A spectrogram is a plot of the amplitude as a function of the time AND the frequency. This is done by taking "windows" of the signal at different time periods and performing a (fast) fourier transform for each of them to get a spectrum. This is called a short-time fourier transform (STFT)
4. A Mel Spectrogram is a spectrogram that is converted into the Mel scale, which scales frequencies to a more human-friendly scale.

We also asked OpenAI's o3-mini model to come up with a plan, and these are the steps:
1. Background Research
- Tacotron to generate mel-spectrograms
- WaveNet (or another vocoder) to convert mel-spectrograms to actual audio files 
2. Data Collection
- There are publically available datasets like VCTK, LibriSpeech.
3. Data Preprocessing, Feature Extraction
- Resampling, Normalization, Noise Reduction techniques to preprocess the data.
- Generating the mel-spectrogram.
4. Learning PyTorch

We also learned (briefly) about autoregressive models (where the output feed back into the input.) This is relevant for the WaveNet architecture from Google Deepmind, which seems like a useful vocoder to use. A vocoder turns mel-spectrograms into audio signals (using an inverse STFT). Apparently this is a hard problem, but me and Liam don't understand why.

We looked at [this blog](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/) for WaveNet and heard some funny samples of TTS (text to speech).

Liam and I are unsure if this is a project worth spending time on. For now, we're going to try a beginning project with PyTorch. See `cifar-10`. This [introductory paper](https://arxiv.org/pdf/1511.08458) is a good resource. We'll read this resource to get a basic understanding of CNNs. We used OpenAI's o3-mini model to generate some starter code as well for CIFAR-10.