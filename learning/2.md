RNNs

An RNN = Recurrent Neural Network. The main differentiator between them and a classic NN is that there are recurrent nodes that can store short term memory about past states and use them to predict future states. These "hidden states" help RNNs handle variable length inputs, although there is a "vanishing/exploding" gradient problem.

What is vanishing gradient problem?
    The vanishing gradient problem is not exclusive to RNNs, but it occurs when the gradient is compounded over multiple layers, with each layer's contribution decreasing during backprop. In RNNs this problem is more apparent because backprop happens over time, which acts like many layers in which the gradient can decrease exponentially. If gradients can go outisde of the range (-1, 1), this becomes the explodinng gradient problem, because the gradients diverge exponentially
Conceptual understanding of RNNs:
    input + hidden layer -> hidden layer, hidden layer -> output
    new hidden layer = activation(W_hx * input + W_hh * hidden layer + bias)
    output = softmax/other activation(W_hy * new hidden layer + another bias)

diagram of RNN structure: https://www.researchgate.net/figure/The-architecture-of-RNN_fig3_312593525
example/inspiration for RNN experiments: https://joshvarty.github.io/VisualizingRNNs/

